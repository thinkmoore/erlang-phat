\documentclass[10pt,letter]{article}

\usepackage{amsmath}
\usepackage{url}
\usepackage{amssymb}
% packages that allow mathematical formatting

\usepackage{graphicx}
% package that allows you to include graphics

\usepackage{setspace}
% package that allows you to change spacing

\usepackage{cite}
% for BibTeX

\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{algorithm}

\usepackage[sc,osf]{mathpazo}
\linespread{1.05}         % Palatino needs more leading
\usepackage[T1]{fontenc}

\fontencoding{T1}
\fontfamily{ppl}
\fontseries{m}
\fontshape{n}
\fontsize{10}{13}
% Set font size. The first parameter is the font size to switch to; the second
% is the \baselineskip to use. The unit of both parameters defaults to pt. A
% rule of thumb is that the baselineskip should be 1.2 times the font size.
\selectfont

\usepackage{parskip}
% Use the style of having no indentation with a space between paragraphs.

\usepackage{enumitem}
% Resume an enumerated list, continuing the old numbering, after some
% intervening text.

% \usepackage{fullpage}
% package that specifies normal margins

\usepackage{titling}
% move the title up a bit, wastes less space
\setlength{\droptitle}{-5em}

\usepackage{caption}
\usepackage{subcaption}
% for subfigures

\usepackage[usenames,dvipsnames]{color}
% for \color used in listings
\usepackage{textcomp}
% for upquote used in listings
\usepackage{listings}
\lstset{
    breakatwhitespace, % somehow prevents lstinline from line splitting
    tabsize=2,
    rulecolor=,
    basicstyle=\footnotesize,
    upquote=true,
    aboveskip={1.5\baselineskip},
    columns=fixed,
    extendedchars=true,
    breaklines=true,
    prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=single,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    keywordstyle=\color{Purple},
    identifierstyle=\color{Black},
    commentstyle=\color{BrickRed},
    stringstyle=\color{RubineRed},
    captionpos=b,
}
\lstloadlanguages{erlang}
\lstset{language=erlang}

\usepackage{tikz}
\usetikzlibrary{backgrounds}
\usetikzlibrary{fit}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.misc} % for cross shaped nodes

\pgfdeclarelayer{b}
\pgfdeclarelayer{f}
\pgfsetlayers{b,main,f}

\newcommand{\plate}[4]{
\begin{pgfonlayer}{b}
\node (invis#1) [draw, color=white, inner sep=1pt,rectangle,fit=#2] {};
\end{pgfonlayer}
\begin{pgfonlayer}{f}
\node (capt#1) [ below left=0 pt of invis#1.south east, xshift=2pt,yshift=1pt, fill=black!15]
{\footnotesize{#3}};
\end{pgfonlayer}
\begin{pgfonlayer}{b}
\node (#1) [fill=black!15, rounded corners, inner sep=5pt, rectangle,fit=(invis#1) (capt#1),#4] {};
\end{pgfonlayer}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Macros

\newcommand{\chubby}[0]{\textsc{Chubby}}
\newcommand{\phat}[0]{\textsc{Phat}}
\newcommand{\phatraid}[0]{\textsc{PhatRaid}}
\newcommand{\phatraidcf}[2]{\textsc{PhatRaid}(#1,#2)}
\newcommand{\raid}[1]{\textsc{RAID #1}}
\newcommand{\paxos}[0]{\textsc{Paxos}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{\phatraid, yo}
\author{Andrew Johnson \and Daniel King \and Lucas Waye \and Scott Moore}
\date{May 13, 2014}

\maketitle

Distributed file stores, like Google's \chubby{}\cite{burrows-chubby}
and CS260r's \phat{}\cite{class}, send all files through a single
master node which consistently replicates the file on many slaves. In
this scheme, the master node's throughput is a bottleneck for storing
large files. We present \phatraid{} which mitigates this bottleneck by
partitioning files across many \paxos{} groups which form a \raid{}\cite{raid}
group.

\section{Introduction}

Google's \chubby{}\cite{burrows-chubby} is a distributed file system which
achieves consensus in the face of limited failures. Unfortunately, for storing
large files disk latency on the cluster nodes is a bottleneck. We propose
building a RAID (Redundant Array of Inexpensive Disks)\cite{raid} group out of distributed file system clusters. Large files are
sharded and disk latency is reduced proportional to number of clusters. Failure
resiliency is not completely sacrificed and we provide an analysis of the
new failure modes.

Our setting is in a data center where we have low network latency, high network
throughput, and the need to dependably store large files with tolerance to some CPU,
network, and disk failure. In this setting, we hypothesize that the major bottleneck
for replicated file storage is disk read/write latency. We combat this
bottleneck with ideas from RAID, which allow for faster file access, and ideas
from distributed consensus algorithms, which allow for consistent replication.

\subsection{Disk Performance}

An important consideration in our project is the impact disk performance has on overall commit latency. We believe that disk read/write latency is the major contributor to storing large files in a distributed system.

In a review of hard disk performance \cite{disk-perf}, Hard Disk Drives (HDD) were compared with Solid State Drives (SSD). HDDs use a physical magnetic platter that spins so that a drive head can read or write data by sensing or manipulating the magnetic field in a particular position on the platter. In contrast, SSDs have no moving parts and instead uses integrated circuits as memory to store data persistently. Because the way in which data is accessed is different, the performance characteristics between the two are different.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{disk-perf}
\caption{{\bf Left Chart}: Solid State Disk reads across entire drive (Left -- Outermost sector, Right -- Innermost sector), {\bf Right Chart}: Hard Disk reads across the entire drive (Left -- Outer rim, Right -- Inner rim). From \cite{disk-perf}}
\label{fig:hdd-vs-sdd}
\end{figure}

Figure \ref{fig:hdd-vs-sdd} shows a comparison between read times for
a SSD and HDD as a function of position on the disk. HDD storage
performance is greatly influenced by where on the disk platter the
data was placed. Data can be placed on the outer rim of the platter
much more quickly than data placed near the center of the platter as
the outer rim is moving faster in comparison (due to conservation of
angular momentum). In contrast to HDDs, SSDs do not exhibit this
problem as there is no spinning platter. The left diagram in Figure
\ref{fig:hdd-vs-sdd} shows relatively constant read performance
throughout the entire disk. Additionally, HDDs have difficultly with
random accesses as the drive may need to wait for the drive head to be
directly over the correct position on the platter. This problem
becomes especially apparent when an individual file is fragmented into
multiple blocks by the file system. SSDs have better random access
times, in general, than HDDs. Sequential access tends to be many times
faster than random access for both SSDs and HDDs \cite{disk-perf}. HDD
performance can also can suffer from {\em spin-up delay}, which
happens as a result of a power-saving measure that stops spinning the
disk during idle times.

We decided that HDDs have far too much performance variability in
order to be properly modeled. As a result, we instead chose to
simulate the performance characteristics of SSDs as they have
relatively constant access times regardless of sector position on the
drive and also do not suffer from {\em spin-up delay}. Also, because
HDDs are slower, and our hypothesis supposes that disk is the
bottleneck, this should be a more challenging case for \phatraid{}.

\urldef\hwbenchmark\url{http://www.tomshardware.com/charts/ssd-charts-2013/benchmarks%2C129.html}
\begin{figure}
\centering
\begin{tabular}{l || c}
\multicolumn{2}{c}{{\bf Solid State Disk Performance Benchmarks}} \\ \hline
Sequential Write Speed  \hspace{7em} & 55.03 - 481.85 MB/sec \\ 
Write Access Time & 0.03 - 0.32 ms \\ \hline
Sequential Read Speed &  207.95 - 522.45 MB/sec \\
Read Access Time & 0.03 - 0.22 ms \\ \hline
\end{tabular}
\caption[Solid State Performance]{Solid State Disk performance for middle 95\% of solid state drives tested in a benchmark. Benchmark available at Benchmark available at \hwbenchmark.}
\label{fig:ssd-perf}
\end{figure}

Figure \ref{fig:ssd-perf} shows the performance for SSDs from a 2013 survey of commercially available SSDs. Sequential write speeds measure the throughput (in megabytes per second) of data stored on the drive when the data is being written in the drive's natural sequential order (e.g. increasing sectors). Similarly, sequential read speed measures the throughput for sequential reads. Read/Write access times represent the latency of performing a single random read or write. 

\section{Implementation}

\subsection{Erlang Primer}

\phatraid{} is implemented in Erlang. Erlang is a pure, functional language with
extensive library support for distributed applications. Erlang has one unit of
concurrency, the \emph{process}, and one unit of distribution, the
\emph{node}. Each Erlang node consists of an arbitrary number of
processes. Erlang nodes may live on the same machine or on different machines
connected by a network. Every Erlang node is given a name by an Erlang name
server. Erlang processes may communicate with any Erlang node or process it can
name. Erlang processes interact with one another by making RPC calls or sending
messages. Identifiers beginning with a lowercase letter are actually
\emph{atoms}, a literal constant similar to
Scheme and Lisp's symbols. Identifiers beginning with an uppercase letter are
normal identifiers.

\subsection{System Organization}

\begin{figure}
\hspace*{-.43in}
  \centering
  \begin{tikzpicture}
    [<->,scale=.85,shorten <=2pt, shorten >=2pt,]
    \node [rectangle, draw, thick, inner sep=5pt] (raidclient) at (0,2) {Raid Client};
    \node [rectangle, draw, thick, inner sep=5pt] (client1) at (-4,1) {Client 1};
    \node [rectangle, draw, thick, inner sep=5pt] (client2) at (0,0) {Client 2};
    \node [rectangle, draw, thick, inner sep=5pt] (client3) at (4,1) {Client 3};
    \node [rectangle, draw, thick, inner sep=5pt] (vr11) at (-6, -2) {Replica};
    \node [rectangle, draw, thick, inner sep=5pt] (vr12) at (-4, -2) {Master};
    \node [rectangle, draw, thick, inner sep=5pt] (vr13) at (-2, -2) {Replica};
    \node [rectangle, draw, thick, inner sep=5pt] (vr21) at (-2, -4) {Replica};
    \node [rectangle, draw, thick, inner sep=5pt] (vr22) at (0, -4) {Master};
    \node [rectangle, draw, thick, inner sep=5pt] (vr23) at (2, -4) {Replica};
    \node [rectangle, draw, thick, inner sep=5pt] (vr31) at (2, -2) {Replica};
    \node [rectangle, draw, thick, inner sep=5pt] (vr32) at (4, -2) {Master};
    \node [rectangle, draw, thick, inner sep=5pt] (vr33) at (6, -2) {Replica};


    \foreach \to in {client1, client2, client3}
      \draw [thick] (raidclient) -- (\to);
    \draw [thick] (client1) -- (vr12);
    \draw [thick] (client2) -- (vr22);
    \draw [thick] (client3) -- (vr32);

    % \foreach \to in {vr11, vr13}
    %   \draw [thick] (vr12) -- (\to);
    % \foreach \to in {vr21, vr23}
    %   \draw [thick] (vr22) -- (\to);
    % \foreach \to in {vr31, vr33}
    %   \draw [thick] (vr32) -- (\to);

    \plate{clientnode}{(raidclient)(client1)(client2)(client3)}{Client Node}{}
    \plate{pc1}{(vr11)(vr12)(vr13)}{Phat Cluster \#1}{}
    \plate{pc2}{(vr21)(vr22)(vr23)}{Phat Cluster \#2}{}
    \plate{pc3}{(vr31)(vr32)(vr33)}{Phat Cluster \#3}{}

  \end{tikzpicture}
  \caption{A \phatraidcf{3}{1} Group}
  \label{fig:organization}
\end{figure}

\phatraid{} is implemented in Erlang, reusing our previous implementation of
\phat{}. The implementation is divided into three domains: the server cluster,
the client, and the raid-client. The server code implements a variant
of the Viewstamped Replication (VR)~\cite{liskov2012viewstamped} consensus
algorithm as well a file system. The client code exposes a file system
API which hides the communication bookkeeping. The raid-client provides a file
system API which transparently partitions files and reassembles them as they are
sent and received from the server cluster.

Figure \ref{fig:organization} depicts the organization of our system. The
raid-client delegates communication to each client. Each client is connected to
one \phat{} cluster, which has a master and a number of replicas.

The master and each replica in a \phat{} cluster maintains a separate
file system. Two different phat clusters do not communicate
directly. The raid-client and client processes all live on one Erlang
node. Each \phat{} master and \phat{} replica lives on its own Erlang
node along with its file system.

\subsection{Disk Emulation and Abstraction Layer}

We developed a novel ``{\bf {\em Disk Emulation and Abstraction Layer}}'' (DEAL)
to simulate SSD performance for our evaluation. The DEAL simulates read and 
write delay using the information from Figure \ref{fig:ssd-perf}. We calculate disk delay as:

\begin{align}
\textsc{ReadDelay}(x) =&\ x / \textsc{SeqReadSpeed} + \textsc{ReadAccessTime}\\
\textsc{WriteDelay}(x) =&\ x / \textsc{SeqWriteSpeed} + \textsc{WriteAccessTime}
\end{align}

where $x$ is the size of the file. During filesystem operations, these formulas are
used to put the process to sleep in proportion to the size of the file being written to
or read from the drive. 

Instead of sending very large files, we sent smaller strings that were treated by the 
DEAL as being much larger (e.g. 1 character represented 1MB of data). As a result,
we extended the DEAL to perform an additional delay for performing bitwise XOR
on a file as that was the only other operation on a file for our tests. 
In our model, drive delays were much greater than XOR delays (e.g. 20 GB/second 
XOR throughput versus 200 MB/second drive throughput).

\subsection{RAIDing \phat{}}

A \phatraidcf{$C$}{$f$} implementation is parameterized by two numbers: $C$, the
number of \phat{} clusters, and $f$, the \phat{} cluster failure tolerance. A
single \phat{} cluster contains $2f + 1$ nodes. A \phatraid{} group contains $C$
clusters for a total of $C(2f+1)$ \phat{} nodes. A \phatraid{} group can be
concurrently accessed by an arbitrary number of raid-clients.

On a raid-client node, $C$ client processes are spawned each of which
maintains a connection to one of the $C$ \phat{} clusters. Each client process
mediates communication between its assigned \phat{} cluster and the raid-client.

A store operation on the raid-client breaks the file into $C-1$ chunks, stores
each chunk on a cluster, and stores a parity bit on the remaining cluster. A
raid-client fetch operation reconstructs the file from the $C-1$ chunks, using
the parity bit if necessary.

\subsection{\phat{} Clusters}

A server cluster is a collection of nodes which maintain a distributed file
system. Each server cluster designates a unique master node (aka the \phat{}
Master). The other nodes are known as \phat{} replicas or just replicas.

The master and each replica have four components: the supervisor, the server,
VR, and the file system:

\begin{itemize}
\item the supervisor -- restarts the other three processes if any dies
\item the server -- passes messages to the VR process if this node is the
  \phat{} master; otherwise, it drops the message and informs the client of the
  current master node
\item VR -- maintains a log of opaque messages, achieving consensus with other
  nodes in the cluster via the Viewstamped Replication protocol
\item the file system -- maintains a hierarchical file system with locks
\end{itemize}

\subsection{Erlang Behaviors}

The four components are implemented using Erlang behaviors. An Erlang behavior
is a framework which implements common patterns like servers and finite-state
machines. In particular, the supervisor uses the \texttt{supervisor} behavior,
the server and file system both use the \texttt{gen\_server} behavior, and VR
uses the \texttt{gen\_fsm} behavior.

\begin{itemize}
\item The \texttt{supervisor} behavior provides a framework for restarting
  failed processes.
\item The \texttt{gen\_server} behavior provides a framework for many-client
  single-server interactions. We implemented functions to process messages
  formatted as Erlang tuples. The \texttt{gen\_server} behavior handles
  low-level socket interaction, messaging queueing, etc.
\item The \texttt{gen\_fsm} (finite-state machine) behavior
generalizes the \texttt{gen\_server} behavior by permitting the server
to have a finite number of states. Each state has a set of functions
with which to process messages. This behavior can be seen as an ad hoc
polymorphic variant of the \texttt{gen\_server} behavior.
\end{itemize}

Additionally, the \texttt{gen\_server} and \texttt{gen\_fsm} behaviors allow the
programmer to specify an arbitrary state value which will be passed around \`{a}
la functional reactive programming. We call this state value the \emph{store}.

\subsubsection{The \texttt{supervisor} Behavior}

\lstinputlisting[firstline=6,lastline=18,float,
                 caption=The \texttt{supervisor} Behavior --- \texttt{phat.erl},
                 label=lst:supervisor,
                 numbers=left, firstnumber=6]
                {../phat.erl}

Listing \ref{lst:supervisor} is taken from our supervisor code. When a
supervisor is started, the \texttt{supervisor} behavior looks for a procedure
called \lstinline!init!. The \lstinline!init! procedure returns a child
specification, which is a pair of a restart specification and a list of
children. In Listing \ref{lst:supervisor}, the restart specification,
\lstinline!{one_for_all,1,5}! specifies that if any one node dies, all nodes
should be restarted, unless more than one failure has occurred in the past five
seconds. If more than one failure occurs in five seconds, the supervisor kills
all its children and then shuts itself down.

Each child in the list of children is a 6-tuple consisting of: child name,
initial procedure call, child transience, shutdown style, child type, and
necessary modules.

\begin{itemize}
\item The initial procedure call is a 3-tuple of module, procedure name and
  argument list.
\item Every child's transience is specified as \lstinline!permanent!, meaning
  they \emph{should} be restarted.
\item Every child has shutdown style \lstinline!1! meaning they will first be
  asked to terminate and one second later they will be forcibly
  terminated\footnote{We could also have specified the hastier shutdown style:
    \lstinline!brutal_kill!}.
\item All of our children are workers, which are distinguished from
  sub-supervisors.
\item The necessary modules are dynamically loaded when the child processes is
  initialized. In \phatraid{}, each child only depends on one module.
\end{itemize}

\subsubsection{The \texttt{gen\_fsm} Behavior}

\begin{lstlisting}[float,caption=The \texttt{gen\_fsm} Behavior --- \texttt{vr.erl},
                   label=lst:genfsm, numbers=left, firstnumber=176]
replica( {prepare, MasterViewNumber, _, _, _, _, _}
       , State = #{ viewNumber := ViewNumber})
    when MasterViewNumber > ViewNumber ->
    ?debugFmt("my view is out of date, need to recover~n", []),
    startRecovery(State);

replica( {prepare, MasterViewNumber, _, _, _, _, _}
       , State = #{ timeout := Timeout, viewNumber := ViewNumber})
    when MasterViewNumber < ViewNumber ->
    ?debugFmt("ignoring prepare from old view~n", []),
    {next_state, replica, State, Timeout};

replica( { prepare, _, Op, OpNumber
         , MasterCommitNumber, Client, RequestNum}
       , State = #{ prepareBuffer := PrepareBuffer }) ->
    Message = {OpNumber, Op, Client, RequestNum},
    NewPrepareBuffer = lists:sort([Message|PrepareBuffer]),
    processPrepareOrCommit( OpNumber
                          , MasterCommitNumber
                          , NewPrepareBuffer
                          , State
                          );
\end{lstlisting}

\begin{lstlisting}[float,caption={Processing the VR Message Queue, Part 1 --- \texttt{vr.erl}},
                   label=lst:processing1, numbers=left, firstnumber=375]
processPrepareOrCommit( OpNumber, MasterCommitNumber, NewPrepareBuffer
                      , State = #{ timeout := Timeout
                                 , commitNumber := CommitNumber
                                 , masterNode := MasterNode
                                 , myNode := MyNode
                                 , viewNumber := ViewNumber
                                 , allNodes := Nodes})
  when CommitNumber > MasterCommitNumber ->
    NewViewNumber = ViewNumber + 1,
    NewMaster = chooseMaster(State, NewViewNumber),
    sendToReplicas( MasterNode
                  , Nodes
                  , {startViewChange, NewViewNumber, MyNode}
                  ),
    { next_state
    , viewChange
    , State#{ viewNumber := NewViewNumbern
            , masterNode := NewMaster }
    , Timeout
    };
\end{lstlisting}
\begin{lstlisting}[float,caption={Processing the VR Message Queue, Part 2 --- \texttt{vr.erl}},
                   label=lst:processing2, numbers=left, firstnumber=397]
processPrepareOrCommit( OpNumber, MasterCommitNumber, NewPrepareBuffer
                      , #{ timeout := Timeout } = State) ->
    AfterBuffer =
      processBuffer( State#{ prepareBuffer := NewPrepareBuffer }
                   , NewPrepareBuffer
                   , MasterCommitNumber
                   ),
    AfterLog = processLog(AfterBuffer, MasterCommitNumber),
    #{ masterNode := MasterNode
     , viewNumber := ViewNumber
     , commitNumber := CommitNumber
     , myNode := MyNode } = AfterLog,
    if
        CommitNumber < MasterCommitNumber ->
            startRecovery(State);
        true ->
            sendToMaster( MasterNode
                        , {prepareOk, ViewNumber, OpNumber, MyNode}
                        ),
            {next_state, replica, AfterLog, Timeout}
    end.
\end{lstlisting}

A state in the \texttt{gen\_fsm} behavior manifests as a procedure which accepts
two arguments: the incoming message and the store. The return value of a
state-procedure is usually the 4-tuple
\lstinline!{next_state,NextState,NewStore,Timeout}!. The \texttt{gen\_fsm}
behavior will transition to \lstinline!NextState! and set the store to
\lstinline!NewStore!. When a new message is received the \texttt{gen\_fsm}
behavior will invoke the \lstinline!NextState! procedure with the new message
and the \lstinline!NewStore!.

The \phat{} file system reaches consensus via the Viewstamped Replication
protocol \cite{liskov2012viewstamped}. Listing \ref{lst:genfsm} defines how to
handle prepare messages sent to a node in the replica state. It is piecewise
defined by pattern matching on the arguments. The first two definition clauses
use the \lstinline!when! keyword to specify arbitrary required relationships
between the matched variables.\footnote{\lstinline{?debugFmt} is a macro
    for printing debug messages when a flag is set}

The final definition clause triggers when the replica receives a message in the
current view. The replica adds the message to a sorted queue and calls a
processing procedure. The processing procedure is defined separately in Listings
\ref{lst:processing1} and \ref{lst:processing2}.

Listing \ref{lst:processing1} handles a message from an out-of-date master,
i.e., the master's commit number is older than the replica's commit number. The
replica first proposes a view change to the cluster. Afterwards, it changes its
\texttt{gen\_fsm} state to \lstinline!viewChange! and updates the
store\footnote{The store is called \lstinline!State! in the code listings} with
the new view number and the new master.

Listing \ref{lst:processing2} handles messages from the current master. The
\lstinline!prepareBuffer! stores prepare messages that arrived out of order. In
particular, if the log ends at operation number $n-1$ and message $n$ is dropped
by the network, all subsequent messages, $n+i$ will be buffered. The buffered
messages will not be added to the log until message $n$ is received.

The call to \lstinline!processBuffer! moves messages to the log, if all previous
messages have now been received. The call to \lstinline!processLog! commits
logged messages which have been newly committed by the master node. Lines
405--408 destructure \lstinline!AfterLog! and bind variables for later use. The
final \lstinline!if! statement responds to the master unless the master has
committed messages the replica has not yet received.\footnote{This can happen if the
  network drops a message whose operation number lies between the replica's last
  commit number and the master's last commit number.}

\section{Evaluation}

\subsection{Resilience}

We distinguish between two types of failure: cessation of progress and loss of
data. In a VR cluster data loss of fully propagated,
commited data only occurs when all $2f+1$ nodes in the cluster fail \footnote{If
  more than $f+1$ nodes die some ``commited but not fully propogated data'' can
  be lost}. In a VR cluster, progress only ceases if $f+1$ nodes are lost. In
\phatraid{} the conditions for cessation of progress and loss of data are
complicated by the combination of RAID and VR.

\begin{figure}
  \centering
  \begin{subfigure}{0.4\textwidth}
    \centering
    \begin{tikzpicture}[<->,scale=.5,shorten <=2pt, shorten >=2pt,]
      \foreach \x/\y in {1/1, 1/2, 1/3, 2/1, 2/2, 2/3, 3/1, 3/2, 3/3}
        \node [circle, draw, thick] (pr\x\y) at (\x,-\y) {};
      \foreach \x/\y in {3/1, 3/2, 3/3, 1/3, 2/3}
        \node [strike out, draw=red, thick] (pr\x\y) at (\x,-\y) {};
    \end{tikzpicture}
    \caption{Progress, No Data Loss}
    \label{fig:failures_allok}
  \end{subfigure}
~
  \begin{subfigure}{0.4\textwidth}
    \centering
    \begin{tikzpicture}[<->,scale=.5,shorten <=2pt, shorten >=2pt,]
      \foreach \x/\y in {1/1, 1/2, 1/3, 2/1, 2/2, 2/3, 3/1, 3/2, 3/3}
        \node [circle, draw, thick] (pr\x\y) at (\x,-\y) {};
      \foreach \x/\y in {1/1, 1/2, 2/1, 2/2}
        \node [strike out, draw=red, thick] (pr\x\y) at (\x,-\y) {};
    \end{tikzpicture}
    \caption{No Progress, No Data Loss}
    \label{fig:failures_noprogress}
  \end{subfigure}
\\\vspace{1em}
  \begin{subfigure}{0.4\textwidth}
    \centering
    \begin{tikzpicture}[scale=.5,shorten <=2pt, shorten >=2pt,]
      \node[rotate=25] (N) at (1.5,-1.5) {\textsc{Impossible!}};
    \end{tikzpicture}
    \caption{Progress, Data Loss}
    \label{fig:failures_impossible}
  \end{subfigure}
~
  \begin{subfigure}{0.4\textwidth}
    \centering
    \begin{tikzpicture}[<->,scale=.5,shorten <=2pt, shorten >=2pt,]
      \foreach \x/\y in {1/1, 1/2, 1/3, 2/1, 2/2, 2/3, 3/1, 3/2, 3/3}
        \node [circle, draw, thick] (pr\x\y) at (\x,-\y) {};
      \foreach \x/\y in {1/1, 1/2, 1/3, 2/1, 2/2, 2/3}
        \node [strike out, draw=red, thick] (pr\x\y) at (\x,-\y) {};
    \end{tikzpicture}
    \caption{No Progress, Data Loss}
    \label{fig:failures_noprogress_dataloss}
  \end{subfigure}

  \caption{Some Failed Node Distributions of \phatraidcf{3}{1}. Each
    circle represent a singe VR master or replica and each column is a
  single \phat{} cluster. The red lines indicate failures.}
  \label{fig:failures}
\end{figure}

Four arrangements of node failures in \phatraidcf{3}{1} are depicted in
Figure \ref{fig:failures}. There are three columns, each representing
a \phat cluster of three nodes. The red lines indicate failures. We
assume failures are independent and uniformly distributed
throughout the \phatraid{} group.

The top left corner, Figure \ref{fig:failures_allok}, depicts five of the nine
nodes failing. This state is \emph{not} a failure state. The group can still
make progress and no data is lost. The two surviving clusters can reach
consensus on new values and can reconstruct the failed cluster's data
(by taking the XOR of the data on the two surviving clusters).

The top right corner, Figure \ref{fig:failures_noprogress}, depicts four of the
nine nodes failing. This state is a failure state. The group cannot make
progress; however, no data can been lost.\footnote{Here and later in this paper
  we mean that no data which has been decided and propagated throughout the
  cluster can be lost. In VR and normal \phat, by contrast, if data is decided and propagated
  throughout the cluster, that data can only be lost if every node is lost.} Progress is
blocked because more than one cluster is unable to reach consensus. Since no cluster
was completely destroyed, all data still exists on at least one node.

The bottom left corner, Figure \ref{fig:failures_impossible}, is impossible
because the group cannot make progress if data has been lost.

The bottom right corner, Figure \ref{fig:failures_noprogress_dataloss}, depicts
six of the nine nodes failing. This state is also a failure state. The group
cannot make progress and will lose data. Progress is blocked because more than
one cluster is unable to reach consensus. Additionally, two complete clusters have
been lost so at least half of the data cannot be recovered.\footnote{If
  only the cluster with the parity bit remains then all useable data
  has been lost}

\subsubsection{Theoretical Analysis}

In general, progress ceases if more than one cluster \emph{cannot reach
  consensus} and data loss occurs if more than one cluster \emph{completely
  fails}. The lower bound of failed nodes for progress cessation in
\phatraidcf{$C$}{$f$} is:

    $$ 2\cdot(f+1) $$

and the lower bound of failed nodes for data loss is:

    $$ 2\cdot(2f+1) $$

The upper bound of failed nodes which still
permit progress is:

    $$ (C-1)\cdot f + 2f + 1 $$

because $C-1$ clusters could still make progress, e.g. if at least $f+1$ nodes are up in
each cluster, and only one cluster is completely dead.

The upper bound for no data loss is:

    $$ (C-1)\cdot 2f + 2f + 1 $$

because it is possible that $C-1$ clusters have at least one node up
and only one cluster has no nodes up.

The bounds presented above cannot fully describe the failure threshold of a
\phatraid{} cluster because of the interplay between VR and RAID. Instead, we
discuss the probability of group failure given $n$ node failures.

Group failure is dependent on the arrangement of failed nodes. For example,
Figure \ref{fig:failures_noprogress} depicts a situation wherein only four
failures stops progress. In contrast, Figure \ref{fig:failures_allok} depicts a
situation wherein five failures does \emph{not} stop progress.

In a \phatraidcf{3}{1} group, less than one-quarter of four-failure arrangements
stop progress. The necessary condition for progress cessation is that two or
more clusters have lost at least $f+1$ nodes.
Only the two-in-two, four-failure arrangement stops progress. We can count the
number of such arrangements:

    $$ \binom{3}{2} \binom{3}{2}^2 = 3^3 = 27 $$

The first term represents choosing the two clusters of three that will
fail. The second term represents choosing which two nodes in each cluster of
three nodes will fail.

The total number of four-failure arrangements is simply:

    $$ \binom{9}{4} = 126 $$

If all nodes fail uniformly and independently, then the probability of four
nodes causing \phatraid{} group failure is

    $$ \frac{27}{126} \approx 0.21 $$

When five nodes fail, they can be distributed into each cluster in three different
ways: (3,1,1), (3,2,0) and (2,2,1). The first case does not stop progress
because only one cluster fails. The later two cases stop progress because two
clusters fail. We calculate the probability of \phatraid{} group failure when
five nodes fail:

\begin{align*}
  \textrm{(3,1,1)}\;\;\;
    &\binom{3}{1}\binom{3}{3}\binom{3}{1}\binom{3}{1} = 3^3 = 27 \\
  \textrm{(3,2,0)}\;\;\;
    &\binom{3}{1}\binom{2}{1}\binom{3}{3}\binom{3}{2} = 2\cdot3^2 = 18 \\
  \textrm{(2,2,1)}\;\;\;
    &\binom{3}{1}\binom{3}{2}\binom{3}{1}\binom{3}{1} = 3^4 = 81 \\
  \textrm{5-Failure Arrs.}\;\;\;
    &\binom{9}{5} = 126 \\
  \textrm{Failure Prob.}\;\;\;
    &\Pr\left[\textrm{group failure} \,|\, \phatraidcf{3}{1},\, n_{failed} = 5\right] \\
    &= \frac{81 + 18}{126} = \frac{99}{126} \approx 0.79 \\
\end{align*}

A VR or PAXOS cluster of size nine is resilient to $f = 4$ failures. In
contrast, \phatraidcf{3}{1} is absolutely resilient to $f=3$ failures, has high
probability of resilience to $f=4$ failures, and has low probability of
resilience to $f=5$ failures.

The general formula for the failure probability distribution is left
as an exercise for the reader, but
it is clear that a \phatraidcf{$C$}{$f$} group is absolutely resilient to $2(f +
1) - 1$ or fewer failures. In addition, a \phatraidcf{$C$}{$f$} group is likely resilient
to $2(f+1)$ failures and unlikely resistant to $(C-1)\cdot f + 2f + 1$ failures.

For exactly $2(f+1)$ node failures, the probability of a \phatraidcf{$C$}{$f$}
group failure is:

\begin{align*}
  \Pr\left[\textrm{group failure} \,|\, \phatraidcf{C}{f},\, n_{failure}=2(f+1)\right]
    =
  \frac{\binom{C}{2}\binom{2f+1}{f+1}^2}
       {\binom{C \cdot (2f+1)}{2(f+1)}}
\end{align*}

For example, for a \phatraidcf{6}{1} we calculate:

\begin{align*}
  P\left[\textrm{group failure} \,|\, \phatraidcf{6}{1},\, n_{failure}=4\right]
    = \frac{135}{3000} \approx 0.05
\end{align*}

\subsection{Performance}

\begin{figure}
  \includegraphics[scale=0.45]{Store.pdf}
  \caption{Scatter Plot of Various \phatraidcf{$C$}{$f$} Configurations}
  \label{fig:plot}
\end{figure}

We evaluated a prototype \phatraidcf{$C$}{$f$} implementation. We simulated disk
latency with sleeps. All experiments are run on a 2013-model MacBook Pro with
8GB of RAM, and a 3GHz Intel Core i7. We found that latency scales as
$\frac{1}{C-1}$. This is the expected behavior for breaking a file into $C-1$
chunks. We always use one bit of party.

Figure \ref{fig:plot} is a scatter plot of the time to store a 100 MB file in a
\phatraid{} group. Each point represents a different configuration. The x-axis
indicates the number of nodes needed for that configuration. The x-axis can also
be seen as a measure of cost to create the configuration.

The slow linear growth from \phatraidcf{1}{1} to \phatraidcf{1}{14} reveals the
cost of communication in large \phat{} clusters. This phenomenon is also present
in the slight increase in cost from \phatraidcf{$C$}{1} to \phatraidcf{$C$}{2}.

As expected \phatraidcf{2}{$f$} does not perform better than non-RAID \phat{}
(i.e. \phatraidcf{1}{$f$}) because the data is not sharded into pieces, and the
parity bit acts as a complete duplicate of the file.

\section{Future Work}

We would like to consider adding more than one parity bit. For some
applications, the diminishing returns on latency are not worth the loss of
failures. In that case, we would do well to add more parity bits, thus allowing
more clusters to completely fail.

We would like to perform a real evaluation rather than a simulation. 
It seems that as file size increases, PhatRaid becomes more appealing 
for lower commit latency. It is not clear from our
simulation, however, when the file size is small enough for other effects (such as
network latency, communications overhead) to become more important.

\section{Conclusion}

We've presented RAID on a distributed file system which shows that sharding
files can improve latency for large files while maintaining reasonable failure
probabilities.

\bibliographystyle{plain}
\bibliography{paper}

\end{document}
