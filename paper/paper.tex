\documentclass[10pt,letter]{article}

\usepackage{amsmath}
\usepackage{url}
\usepackage{amssymb}
% packages that allow mathematical formatting

\usepackage{graphicx}
% package that allows you to include graphics

\usepackage{setspace}
% package that allows you to change spacing

\usepackage{cite}
% for BibTeX

\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{algorithm}

\usepackage[sc,osf]{mathpazo}
\linespread{1.05}         % Palatino needs more leading
\usepackage[T1]{fontenc}

\fontencoding{T1}
\fontfamily{ppl}
\fontseries{m}
\fontshape{n}
\fontsize{10}{13}
% Set font size. The first parameter is the font size to switch to; the second
% is the \baselineskip to use. The unit of both parameters defaults to pt. A
% rule of thumb is that the baselineskip should be 1.2 times the font size.
\selectfont

\usepackage{parskip}
% Use the style of having no indentation with a space between paragraphs.

\usepackage{enumitem}
% Resume an enumerated list, continuing the old numbering, after some
% intervening text.

% \usepackage{fullpage}
% package that specifies normal margins

\usepackage{titling}
% move the title up a bit, wastes less space
\setlength{\droptitle}{-5em}

\usepackage{caption}
\usepackage{subcaption}
% for subfigures

\usepackage[usenames,dvipsnames]{color}
% for \color used in listings
\usepackage{textcomp}
% for upquote used in listings
\usepackage{listings}
\lstset{
    breakatwhitespace, % somehow prevents lstinline from line splitting
    tabsize=2,
    rulecolor=,
    basicstyle=\footnotesize,
    upquote=true,
    aboveskip={1.5\baselineskip},
    columns=fixed,
    extendedchars=true,
    breaklines=true,
    prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=single,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    keywordstyle=\color{Purple},
    identifierstyle=\color{Black},
    commentstyle=\color{BrickRed},
    stringstyle=\color{RubineRed},
    captionpos=b,
}
\lstloadlanguages{erlang}
\lstset{language=erlang}

\usepackage{tikz}
\usetikzlibrary{backgrounds}
\usetikzlibrary{fit}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.misc} % for cross shaped nodes

\pgfdeclarelayer{b}
\pgfdeclarelayer{f}
\pgfsetlayers{b,main,f}

\newcommand{\plate}[4]{
\begin{pgfonlayer}{b}
\node (invis#1) [draw, color=white, inner sep=1pt,rectangle,fit=#2] {};
\end{pgfonlayer}
\begin{pgfonlayer}{f}
\node (capt#1) [ below left=0 pt of invis#1.south east, xshift=2pt,yshift=1pt, fill=black!15]
{\footnotesize{#3}};
\end{pgfonlayer}
\begin{pgfonlayer}{b}
\node (#1) [fill=black!15, rounded corners, inner sep=5pt, rectangle,fit=(invis#1) (capt#1),#4] {};
\end{pgfonlayer}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Macros

\newcommand{\chubby}[0]{\textsc{Chubby}}
\newcommand{\phat}[0]{\textsc{Phat}}
\newcommand{\phatraid}[0]{\textsc{PhatRaid}}
\newcommand{\phatraidcf}[2]{\textsc{PhatRaid}(#1,#2)}
\newcommand{\raid}[1]{\textsc{RAID #1}}
\newcommand{\paxos}[0]{\textsc{Paxos}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{PHAT RAID, yo}
\author{Andrew Johnson \and Daniel King \and Lucas Waye \and Scott Moore}
\date{May 13, 2014}

\maketitle

Distributed file stores, like Google's \chubby{} and CS260r's \phat{}, send all
files through a single master node which consistently replicates the file on
many slaves. In this scheme, the master node's throughput is a bottleneck for
storing large files. We present \phatraid{} which mitigates this bottleneck by
partitioning files across many \paxos{} groups which form a \raid{1} group.

\section{Introduction}

Google's \chubby{}\cite{burrows-chubby} is a distributed file system which
achieves consensus in the face of limited failures. Unfortunately, for storing
large files disk latency on the cluster nodes is a bottleneck. We propose
building a RAID group out of distributed file system clusters. Large files are
sharded and disk latency is reduced proportional to number of clusters. Failure
resiliency is not completely sacrificed and we provide an analysis of the
new failure modes.

\subsection{Disk Performance}

An important consideration in our project is the impact disk performance has on overall commit latency. We believe that disk read/write latency is the major contributor to storing large files in a distributed system. {\bf TODO more}

In a review of hard disk performance \cite{disk-perf}, Hard Disk Drives (HDD) were compared with Solid State Drives (SSD). HDDs use a physical magnetic platter that spins so that a drive head can read or write data by sensing or manipulating the magnetic field in a particular position on to the platter. In contrast, SSDs have no moving parts and instead uses integrated circuits as memory to store data persistently. Because the way in which data is accessed is different, the performance characteristics between the two are different.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{disk-perf}
\caption{{\bf Left Chart}: Solid State Disk reads across entire drive (Left -- Outermost sector, Right -- Innermost sector), {\bf Right Chart}: Hard Disk reads across the entire drive (Left -- Outer rim, Right -- Inner rim). From \cite{disk-perf}}
\label{fig:hdd-vs-sdd}
\end{figure}

Figure \ref{fig:hdd-vs-sdd} shows a comparison between read times for a SSD and HDD as a function of position on the disk. HDD storage performance is greatly influenced by where on the disk platter the data was placed. Data can be placed on the outer rim of the platter much more quickly than data placed near the center of the platter as the outer rim is moving faster in comparison (due to conservation of angular momentum). In contrast to HDDs, SSDs do not exhibit this problem as there is no spinning platter. The right diagram in \ref{fig:hdd-vs-sdd} shows relatively constant read performance throughout the entire disk. Additionally, HDDs have difficultly with random accesses as the drive may need to wait for the drive head to be directly over the correct position on the platter. This problem becomes especially apparent when an individual file is fragmented into multiple blocks by the file system. SSDs have relatively better random access times, in general, than HDDs. Sequential access tends to be many times faster than random access for both SSDs and HDDs \cite{disk-perf}. HDD performance can also can suffer from {\em spin-up delay}, which happens as a result of a power-saving measure that stops spinning the disk during idle times.

We decided that HDDs have far too much performance variability in order to be properly modeled. As a result, we instead chose to simulate the performance characteristics of SSDs as they have relatively constant access times regardless of sector position on the drive and also do not suffer from {\em spin-up delay}.

\urldef\hwbenchmark\url{http://www.tomshardware.com/charts/ssd-charts-2013/benchmarks%2C129.html}
\begin{figure}
\centering
\begin{tabular}{l || c}
\multicolumn{2}{c}{{\bf Solid State Disk Performance Benchmarks}} \\ \hline
Sequential Write Speed  \hspace{7em} & 55.03 - 481.85 MB/sec \\ 
Write Access Time & 0.03 - 0.32 ms \\ \hline
Sequential Read Speed &  207.95 - 522.45 MB/sec \\
Read Access Time & 0.03 - 0.22 ms \\ \hline
\end{tabular}
\caption[Solid State Performance]{Solid State Disk performance for middle 95\% of solid state drives tested in a benchmark. Benchmark available at Benchmark available at \hwbenchmark.}
\label{fig:sdd-perf}
\end{figure}

Figure \ref{fig:sdd-perf} shows the performance for SSDs from a 2013 survey of commercially available SSDs. Sequential write speeds measure the throughput (in megabytes per second) of data stored on the drive when the data is being written in the drive's natural sequential order (e.g. increasing sectors). Similarly, sequential read speed measures the throughput for sequential reads. Read/Write access times represent the latency of performing a single random read or write. 

\section{Implementation}

\subsection{Erlang Primer}

\phatraid{} is implemented in Erlang. Erlang is a pure, functional language with
extensive library support for distributed applications. Erlang has one unit of
concurrency, the \emph{process}, and one unit of distribution, the
\emph{node}. Each Erlang node consists of an arbitrary number of
processes. Erlang nodes may live on the same machine or on different machines
connected by a network. Every Erlang node is given a name by an Erlang name
server. Erlang processes may communicate with any Erlang node or process it can
name. Erlang processes interact with one another by making RPC calls or sending
messages. Identifiers beginning with a lowercase letter are actually atoms, like
Scheme's symbols. Identifiers beginning with an uppercase letter are normal
identifiers.

\subsection{System Organization}

\begin{figure}
  \centering
  \begin{tikzpicture}
    [<->,scale=.85,shorten <=2pt, shorten >=2pt,]
    \node [rectangle, draw, thick, inner sep=5pt] (raidclient) at (0,2) {Raid Client};
    \node [rectangle, draw, thick, inner sep=5pt] (client1) at (-4,1) {Client 1};
    \node [rectangle, draw, thick, inner sep=5pt] (client2) at (0,0) {Client 2};
    \node [rectangle, draw, thick, inner sep=5pt] (client3) at (4,1) {Client 3};
    \node [rectangle, draw, thick, inner sep=5pt] (vr11) at (-6, -2) {Replica};
    \node [rectangle, draw, thick, inner sep=5pt] (vr12) at (-4, -2) {Master};
    \node [rectangle, draw, thick, inner sep=5pt] (vr13) at (-2, -2) {Replica};
    \node [rectangle, draw, thick, inner sep=5pt] (vr21) at (-2, -4) {Replica};
    \node [rectangle, draw, thick, inner sep=5pt] (vr22) at (0, -4) {Master};
    \node [rectangle, draw, thick, inner sep=5pt] (vr23) at (2, -4) {Replica};
    \node [rectangle, draw, thick, inner sep=5pt] (vr31) at (2, -2) {Replica};
    \node [rectangle, draw, thick, inner sep=5pt] (vr32) at (4, -2) {Master};
    \node [rectangle, draw, thick, inner sep=5pt] (vr33) at (6, -2) {Replica};


    \foreach \to in {client1, client2, client3}
      \draw [thick] (raidclient) -- (\to);
    \draw [thick] (client1) -- (vr12);
    \draw [thick] (client2) -- (vr22);
    \draw [thick] (client3) -- (vr32);

    % \foreach \to in {vr11, vr13}
    %   \draw [thick] (vr12) -- (\to);
    % \foreach \to in {vr21, vr23}
    %   \draw [thick] (vr22) -- (\to);
    % \foreach \to in {vr31, vr33}
    %   \draw [thick] (vr32) -- (\to);

    \plate{clientnode}{(raidclient)(client1)(client2)(client3)}{Client Node}{}
    \plate{pc1}{(vr11)(vr12)(vr13)}{Phat Cluster \#1}{}
    \plate{pc2}{(vr21)(vr22)(vr23)}{Phat Cluster \#2}{}
    \plate{pc3}{(vr31)(vr32)(vr33)}{Phat Cluster \#3}{}

  \end{tikzpicture}
  \caption{A \phatraidcf{3}{1} Group}
  \label{fig:organization}
\end{figure}

\phatraid{} is implemented in Erlang, reusing our previous implementation of
\phat{}. The implementation is divided into three domains: the server cluster,
the client, and the raid-client. The server code implements a VR-like consensus
algorithm as well as simple file system. The client code exposes a file system
API which hides the communication bookkeeping. The raid-client provides a file
system API which transparently partitions files and reassembles them as they are
sent and received from the server cluster.

Figure \ref{fig:organization} depicts the organization of our system. The
raid-client delegates communication to each client. Each client is connected to
one \phat{} cluster, which has a master and a number of replicas.

Each \phat{} cluster maintains a file system which is separate and distinct from
the file systems of other \phat{} clusters. Generally, two phat clusters do not
communicate. The raid-client and client processes all live on one node. Each
\phat{} master and \phat{} replica lives on its own node.

\subsection{Disk Emulation and Abstraction Layer}

We developed a novel ``{\bf {\em Disk Emulation and Abstraction Layer}}'' (DEAL)
to simulate SSD performance so they .

\subsection{RAIDing \phat{}}

A \phatraidcf{$C$}{$f$} implementation is parameterized by two numbers: $C$, the
number of \phat{} clusters, and $f$, the \phat{} cluster failure tolerance. A
single \phat{} cluster contains $2f + 1$ nodes. A \phatraid{} group contains $C$
clusters for a total of $C(2f+1)$ \phat{} nodes. A \phatraid{} group can be
concurrently accessed by an arbitrary number of raid-clients.

On a client node, the raid-client spawns $C$ client processes which each
maintain a connection to one of the $C$ \phat{} clusters. Each client process
mediates all communication between its assigned \phat{} cluster and the
raid-client.

A store operation on the raid-client breaks the file into $C-1$ chunks, stores
each chunk on a cluster, and stores a parity bit on the remaining cluster. A
raid-client fetch operation reconstructs the file from the $C-1$ chunks, using
the parity bit if necessary.

\subsection{Erlang Behaviors}

The server cluster is a collection of nodes running the same server code. One of
these nodes is designated the master node (aka \phat{} Master). The other nodes are
known as \phat{} replicas or just replicas.

The master and each replica has four components: the supervisor, the
(unfortunately named) server, VR, and the file system:

\begin{itemize}
\item the supervisor -- restarts the other three processes if any single node dies
\item the server -- passes messages to the VR process if this node is the
  \phat{} master; otherwise, it drops the message and informs the client of the
  current master node
\item VR -- maintains a log of opaque messages, achieving consensus with other
  nodes in the cluster via the Viewstamped Replication protocol
\item the file system -- maintains a simple hierarchical file system with locks
\end{itemize}

The four components are implemented using Erlang behaviors. An Erlang behavior
is a framework which implements common patterns like servers and finite state
machines. In particular, the supervisor uses the \texttt{supervisor} behavior,
the server and file system both use the \texttt{gen\_server} behavior, and VR
uses the \texttt{gen\_fsm} behavior.

\begin{itemize}
\item The \texttt{supervisor} behavior provides a framework for restarting
  failed processes.
\item The \texttt{gen\_server} behavior provides a framework for many-client
  single-server interactions. We implemented functions to process messages
  formatted as Erlang tuples. The \texttt{gen\_server} behavior handles
  low-level socket interaction, messaging queueing, etc.
\item The \texttt{gen\_fsm} behavior generalizes the \texttt{gen\_server}
  behavior by permitting the server to have a finite number of states. Each
  state has a set of functions with which to process messages. This behavior can
  be seen as an ad hoc polymorphic variant of the \texttt{gen\_server} behavior.
\end{itemize}

Additionally, the \texttt{gen\_server} and \texttt{gen\_fsm} behaviors allow
the programmer to specify an arbitrary state value which will be passed around
\`{a} la functional reactive programming.

\subsubsection{The \texttt{supervisor} Behavior}

\lstinputlisting[firstline=6,lastline=18,float,
                 caption=The \texttt{supervisor} Behavior --- \texttt{phat.erl},
                 label=lst:supervisor,
                 numbers=left, firstnumber=6]
                {../phat.erl}

Listing \ref{lst:supervisor} is taken from our supervisor code. When a
supervisor is started, the \texttt{supervisor} behavior looks for a unary
procedure called \lstinline!init!. The \lstinline!init! procedure returns a
child specification which is a pair of a restart specification and a list of
children. In Listing \ref{lst:supervisor}, the restart specification,
\lstinline!{one_for_all,1,5}! specifies that if any one node dies, all nodes
should be restarted, unless more than one failure has occurred in the past five
seconds. If more than one failure occurs in five seconds, the supervisor kills
all the children and shuts itself down as well.

Each child in the list of children is a 6-tuple consisting of: child name,
initial procedure call, child transience, shutdown style, child type, and
necessary modules.

\begin{itemize}
\item The initial procedure call is a 3-tuple of module, procedure name and
  argument list.
\item Every child's transience is specified as \lstinline!permanent!, meaning
  they \emph{should} be restarted.
\item Every child has shutdown style \lstinline!1! meaning they will first be
  asked to terminate and one second later they will be forcibly
  terminated\footnote{We could also have specified the hastier shutdown style:
    \lstinline!brutal_kill!}.
\item All of our children are workers, which are distinguished from
  sub-supervisors.
\item The necessary modules are dynamically loaded when the child processes is
  initialized. In \phatraid{}, each child only depends on one module.
\end{itemize}

\subsubsection{The \texttt{gen\_fsm} Behavior}

\begin{lstlisting}[float,caption=The \texttt{gen\_fsm} Behavior --- \texttt{vr.erl},
                   label=lst:genfsm, numbers=left, firstnumber=176]
replica( {prepare, MasterViewNumber, _, _, _, _, _}
       , State = #{ viewNumber := ViewNumber})
    when MasterViewNumber > ViewNumber ->
    ?debugFmt("my view is out of date, need to recover~n", []),
    startRecovery(State);

replica( {prepare, MasterViewNumber, _, _, _, _, _}
       , State = #{ timeout := Timeout, viewNumber := ViewNumber})
    when MasterViewNumber < ViewNumber ->
    ?debugFmt("ignoring prepare from old view~n", []),
    {next_state, replica, State, Timeout};

replica( { prepare, _, Op, OpNumber
         , MasterCommitNumber, Client, RequestNum}
       , State = #{ prepareBuffer := PrepareBuffer }) ->
    Message = {OpNumber, Op, Client, RequestNum},
    NewPrepareBuffer = lists:sort([Message|PrepareBuffer]),
    processPrepareOrCommit( OpNumber
                          , MasterCommitNumber
                          , NewPrepareBuffer
                          , State
                          );
\end{lstlisting}

\begin{lstlisting}[float,caption={Processing the VR Message Queue, Part 1 --- \texttt{vr.erl}},
                   label=lst:processing1, numbers=left, firstnumber=375]
processPrepareOrCommit( OpNumber, MasterCommitNumber, NewPrepareBuffer
                      , State = #{ timeout := Timeout
                                 , commitNumber := CommitNumber
                                 , masterNode := MasterNode
                                 , myNode := MyNode
                                 , viewNumber := ViewNumber
                                 , allNodes := Nodes})
  when CommitNumber > MasterCommitNumber ->
    NewViewNumber = ViewNumber + 1,
    NewMaster = chooseMaster(State, NewViewNumber),
    sendToReplicas( MasterNode
                  , Nodes
                  , {startViewChange, NewViewNumber, MyNode}
                  ),
    { next_state
    , viewChange
    , State#{ viewNumber := NewViewNumbern
            , masterNode := NewMaster }
    , Timeout
    };
\end{lstlisting}
\begin{lstlisting}[float,caption={Processing the VR Message Queue, Part 2 --- \texttt{vr.erl}},
                   label=lst:processing2, numbers=left, firstnumber=397]
processPrepareOrCommit( OpNumber, MasterCommitNumber, NewPrepareBuffer
                      , #{ timeout := Timeout } = State) ->
    AfterBuffer =
      processBuffer( State#{ prepareBuffer := NewPrepareBuffer }
                   , NewPrepareBuffer
                   , MasterCommitNumber
                   ),
    AfterLog = processLog(AfterBuffer, MasterCommitNumber),
    #{ masterNode := MasterNode
     , viewNumber := ViewNumber
     , commitNumber := CommitNumber
     , myNode := MyNode } = AfterLog,
    if
        CommitNumber < MasterCommitNumber ->
            startRecovery(State);
        true ->
            sendToMaster( MasterNode
                        , {prepareOk, ViewNumber, OpNumber, MyNode}
                        ),
            {next_state, replica, AfterLog, Timeout}
    end.
\end{lstlisting}

A state in the \texttt{gen\_fsm} behavior manifests as a procedure which accepts
two arguments: the incoming message and the store\footnote{Mutable state is
  implemented as a pure store value which is passed around by the behavior}. The
return value of a state-procedure is usually the 4-tuple
\lstinline!{next_state,NextState,NewStore,Timeout}!. The \texttt{gen\_fsm}
behavior will transition to \lstinline!NextState! and set the store to
\lstinline!NewStore!. When a new message is received the \texttt{gen\_fsm}
behavior will invoke the \lstinline!NextState! procedure with the new message
and the \lstinline!NewStore!.

The \phat{} file system achieves consensus via the Viewstamped Replication
protocol \cite{liskov2012viewstamped}. Listing \ref{lst:genfsm} defines how to
handle prepare messages sent to a node in the replica state. It is piecewise
defined by pattern matching on the arguments. The first two definition clauses
use the \lstinline!when! keyword to specify arbitrary required relationships
between the matched variables.

The final definition clause triggers when the replica receives a message in the
current view. The replica adds the message to a sorted queue and calls a
processing procedure. The processing procedure is defined separately in Listings
\ref{lst:processing1} and \ref{lst:processing2}.

Listing \ref{lst:processing1} handles a message from an out-of-date master,
i.e., the master's commit number is older than the replica's commit number. The
replica first proposes a view change to the cluster. Afterwards, it changes its
\texttt{gen\_fsm} state to \lstinline!viewChange! and updates the
store\footnote{The store is called \lstinline!State! in the code listings} with
the new view number and the new master.

Listing \ref{lst:processing2} handles messages from the current master. The
\lstinline!prepareBuffer! stores prepare messages that arrived out of order. In
particular, if the log ends at operation number $n$ and message $n+1$ is dropped
by the network, all subsequent messages will be buffered. The buffered messages
will not be added to the log until message $n+1$ is received. The call to
\lstinline!processBuffer! moves messages to the log, if all previous messages
have now been received. The call to \lstinline!processLog! commits logged
messages which now have been committed by the master node. Lines 405 to 408
destructure \lstinline!AfterLog! and bind variables for later use. The final
\lstinline!if! statement responds to the master unless the master has committed
messages we haven't received.\footnote{This can happen if the network drops a
  message whose operation number lies between the replica's last commit number
  and the master's last commit number.}

\section{Evaluation}

\subsection{Resilience}

We distinguish between two types of failure: cessation of progress and loss of
data. In a VR cluster without persistent stores, data loss of fully propagated,
commited data only occurs when all $2f+1$ nodes in the cluster fail \footnote{If
  more than $f+1$ nodes die some ``commited but not fully propogated data'' can
  be lost}. In a VR cluster, progress only ceases if $f+1$ nodes are lost. In
\phatraid{} the conditions for cessation of progress and loss of data are
complicated by the combination of RAID and VR.

\begin{figure}
  \centering
  \begin{subfigure}{0.4\textwidth}
    \centering
    \begin{tikzpicture}[<->,scale=.5,shorten <=2pt, shorten >=2pt,]
      \foreach \x/\y in {1/1, 1/2, 1/3, 2/1, 2/2, 2/3, 3/1, 3/2, 3/3}
        \node [circle, draw, thick] (pr\x\y) at (\x,-\y) {};
      \foreach \x/\y in {3/1, 3/2, 3/3, 1/3, 2/3}
        \node [strike out, draw=red, thick] (pr\x\y) at (\x,-\y) {};
    \end{tikzpicture}
    \caption{Progress, No Data Loss}
    \label{fig:failures_allok}
  \end{subfigure}
~
  \begin{subfigure}{0.4\textwidth}
    \centering
    \begin{tikzpicture}[<->,scale=.5,shorten <=2pt, shorten >=2pt,]
      \foreach \x/\y in {1/1, 1/2, 1/3, 2/1, 2/2, 2/3, 3/1, 3/2, 3/3}
        \node [circle, draw, thick] (pr\x\y) at (\x,-\y) {};
      \foreach \x/\y in {1/1, 1/2, 2/1, 2/2}
        \node [strike out, draw=red, thick] (pr\x\y) at (\x,-\y) {};
    \end{tikzpicture}
    \caption{No Progress, No Data Loss}
    \label{fig:failures_noprogress}
  \end{subfigure}
\\\vspace{1em}
  \begin{subfigure}{0.4\textwidth}
    \centering
    \begin{tikzpicture}[scale=.5,shorten <=2pt, shorten >=2pt,]
      \node[rotate=25] (N) at (1.5,-1.5) {\textsc{Impossible!}};
    \end{tikzpicture}
    \caption{Progress, Data Loss}
    \label{fig:failures_impossible}
  \end{subfigure}
~
  \begin{subfigure}{0.4\textwidth}
    \centering
    \begin{tikzpicture}[<->,scale=.5,shorten <=2pt, shorten >=2pt,]
      \foreach \x/\y in {1/1, 1/2, 1/3, 2/1, 2/2, 2/3, 3/1, 3/2, 3/3}
        \node [circle, draw, thick] (pr\x\y) at (\x,-\y) {};
      \foreach \x/\y in {1/1, 1/2, 1/3, 2/1, 2/2, 2/3}
        \node [strike out, draw=red, thick] (pr\x\y) at (\x,-\y) {};
    \end{tikzpicture}
    \caption{No Progress, Data Loss}
    \label{fig:failures_noprogress_dataloss}
  \end{subfigure}

  \caption{Some Failed Node Distributions of \phatraidcf{3}{1}}
  \label{fig:failures}
\end{figure}

Four arrangements of failed nodes in \phatraidcf{3}{1} are depicted in Figure
\ref{fig:failures}. In this section, if we claim ``data is lost'', we mean that
if no persistent store is used, data will be lost. Adding a persistent store
obviously prevents loss of committed data. Additionally, we assume node failures
are independent and uniformly distributed throughout the \phatraid{} group.

The top left corner, Figure \ref{fig:failures_allok}, depicts five of the nine
nodes failing. This state is \emph{not} a failure state. The group can still
make progress and no data is lost. The two surviving clusters can reach
consensus on new values and can reconstruct the failed cluster's data.

The top right corner, Figure \ref{fig:failures_noprogress}, depicts four of the
nine nodes failing. This state is a failure state. The group cannot make
progress; however, no data can been lost. Progress is blocked because more than
one cluster cannot reach consensus. Since no cluster was completely destroyed,
all data still exists on at least one node.

The bottom left corner, Figure \ref{fig:failures_impossible}, is impossible
because if data can be lost then we certainly cannot make progress.

The bottom right corner, Figure \ref{fig:failures_noprogress_dataloss}, depicts
six of the nine nodes failing. This state is also a failure state. The group
cannot make progress and will lose data. Progress is blocked because more than
one cluster cannot reach consensus. Additionally, two complete clusters have
been lost so half of the data cannot be recovered.

\subsubsection{Theoretical Analysis}

In general, progress ceases if more than one cluster \emph{cannot reach
  consensus} and data loss occurs if more than one \emph{cluster completely
  fails}. The lower bound of failed nodes for progress cessation in
\phatraidcf{$C$}{$f$} is:

    $$ 2\cdot(f+1) $$

and the lower bound of failed nodes for data loss is:

    $$ 2\cdot(2f+1) $$

By the pidgeon hole principle, the upper bound of failed nodes which still
permit progress is,

    $$ C\cdot f + 1 $$

because we can place $f$ failures in all but one cluster and place $f+1$
failures in the final cluster.

The upper bound for no data loss is,

    $$ C\cdot 2f + 1 $$

because we can keep one node of all but one cluster alive. From these $C-1$
nodes we can reconstruct the entire file system.

The bounds presented above cannot fully describe the failure threshold of a
\phatraid{} cluster because of the interplay between VR and RAID. Instead, we
will discuss the probability of group failure given $n$ node failures.

Group failure is dependent on the arrangement of failed nodes. For example,
Figure \ref{fig:failures_noprogress} depicts a situation wherein only four
failures stops progress. In contrast, Figure \ref{fig:failures_allok} depicts a
situation wherein five failures does \emph{not} stop progress.

In a \phatraidcf{3}{1} group, less than one-quarter of four-failure arrangements
stop progress. The necessary condition for progress cessation is that two or
more clusters have lost at least $f+1$ nodes. By the pidgeon hole principle,
only the two-in-two, four-failure arrangement stops progress. We can count the
number of such arrangements:

    $$ \binom{3}{2} \binom{3}{2}^2 = 3^3 = 27 $$

The first term represents choosing the two clusters of three that will
fail. The second term represents choosing which two nodes in each cluster of
three nodes will fail.

The total number of four-failure arrangements is simply:

    $$ \binom{9}{4} = 126 $$

If all nodes fail uniformly and independently, then the probability of four
nodes causing \phatraid{} group failure is

    $$ \frac{27}{126} \approx 0.21 $$

When five nodes fail, they can be distributed into each group: (3,1,1), (3,2,0)
and (2,2,1). The first case does not stop progress because only one cluster
fails. The later two cases stop progress because two clusters fail. We calculate
the probability of \phatraid{} group failure when five nodes fail:

\begin{align*}
  \textrm{(3,1,1)}\;\;\;
    &\binom{3}{1}\binom{3}{3}\binom{3}{1}\binom{3}{1} = 3^3 = 27 \\
  \textrm{(3,2,0)}\;\;\;
    &\binom{3}{1}\binom{2}{1}\binom{3}{3}\binom{3}{2} = 2\cdot3^2 = 18 \\
  \textrm{(2,2,1)}\;\;\;
    &\binom{3}{1}\binom{3}{2}\binom{3}{1}\binom{3}{1} = 3^4 = 81 \\
  \textrm{5-Failure Arrs.}\;\;\;
    &\binom{9}{5} = 126 \\
  \textrm{Failure Prob.}\;\;\;
    &\Pr\left[\textrm{group failure} \,|\, \phatraidcf{3}{1},\, n_{failed} = 5\right] \\
    &= \frac{81 + 18}{126} = \frac{99}{126} \approx 0.79 \\
\end{align*}

A VR or PAXOS cluster of size nine is resilient to $f = 4$ failures. In
contrast, \phatraidcf{3}{1} is absolutely resilient to $f=3$ failures, has high
probability of resilience to $f=4$ failures, and has low probability of
resilience to $f=5$ failures.

The general formula for the failure probability distribution is not clear, but
it is clear that a \phatraidcf{$C$}{$f$} group is absolutely resilient to $2(f +
1) - 1$ or fewer failures. A \phatraidcf{$C$}{$f$} group is likely resilient
to $2(f+1)$ failures and unlikely resistant to $C\cdot f + 1$ failures.

For exactly $2(f+1)$ node failures, the probability of \phatraidcf{$C$}{$f$}
group failure is:

\begin{align*}
  \Pr\left[\textrm{group failure} \,|\, \phatraidcf{C}{f},\, n_{failure}=2(f+1)\right]
    =
  \frac{\binom{C}{2}\binom{2f+1}{f+1}^2}
       {\binom{C \cdot (2f+1)}{2(f+1)}}
\end{align*}

For \phatraidcf{6}{1} we calculate:

\begin{align*}
  P\left[\textrm{group failure} \,|\, \phatraidcf{6}{1},\, n_{failure}=4\right]
    = \frac{135}{3000} \approx 0.05
\end{align*}

\subsection{Performance}

\begin{figure}
  \includegraphics[scale=0.45]{Store.pdf}
  \caption{Scatter Plot of Various \phatraidcf{$C$,$f$} Configurations}
  \label{fig:plot}
\end{figure}

We evaluated a prototype \phatraidcf{C}{f} implementation. We simulated disk
latency with sleeps. All experiments are run on a 2013-model MacBook Pro with
8GB of RAM, and a 3GHz Intel Core i7. We found that latency scales as
$\frac{1}{C-1}$. This is the expected behavior for breaking a file into $C-1$
chunks. We always use one bit of parity.

As expected \phatraidcf{2}{$f$} does not perform better than non-RAID \phat{}
(i.e. \phatraidcf{1}{$f$}) because the data is not sharded into pieces, and the
parity bit acts as a complete duplicate of the file.

\section{Future Work}

We would like to consider adding more than one parity bit. For some
applications, the diminishing returns on latency are not worth the loss of
failures. In that case, we would do well to add more parity bits, thus allowing
more clusters to completely fail.

\section{Conclusion}

We've presented RAID on a distributed file system which shows that sharding
files improves latency for large files while maintaining reasonable failure
probabilities.

\bibliographystyle{plain}
\bibliography{paper}

\end{document}
